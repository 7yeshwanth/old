{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34l1vR1toIAk",
        "outputId": "f228c7ff-80ea-4baa-affc-49e4dc1d1940"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Configuration 1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config 1:\n",
            "  Layers: (100,), Activation: relu, Solver: adam\n",
            "  Training Accuracy: 96.79%\n",
            "  Testing Accuracy: 94.23%\n",
            "  Convergence Time: 26.32 seconds\n",
            "\n",
            "Training Configuration 2...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config 2:\n",
            "  Layers: (100, 50), Activation: relu, Solver: adam\n",
            "  Training Accuracy: 96.32%\n",
            "  Testing Accuracy: 93.99%\n",
            "  Convergence Time: 28.72 seconds\n",
            "\n",
            "Training Configuration 3...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config 3:\n",
            "  Layers: (100,), Activation: relu, Solver: sgd\n",
            "  Training Accuracy: 92.31%\n",
            "  Testing Accuracy: 91.04%\n",
            "  Convergence Time: 24.83 seconds\n",
            "\n",
            "Training Configuration 4...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config 4:\n",
            "  Layers: (200, 100), Activation: relu, Solver: adam\n",
            "  Training Accuracy: 98.48%\n",
            "  Testing Accuracy: 95.96%\n",
            "  Convergence Time: 47.05 seconds\n",
            "\n",
            "Training Configuration 5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config 5:\n",
            "  Layers: (50,), Activation: relu, Solver: adam\n",
            "  Training Accuracy: 94.80%\n",
            "  Testing Accuracy: 93.00%\n",
            "  Convergence Time: 20.67 seconds\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import time\n",
        "\n",
        "# Load and preprocess data\n",
        "mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
        "X, y = mnist[\"data\"], mnist[\"target\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=10000, random_state=42)\n",
        "\n",
        "configs = [\n",
        "    {'hidden_layer_sizes': (100,), 'activation': 'relu', 'solver': 'adam'},\n",
        "    {'hidden_layer_sizes': (100, 50), 'activation': 'relu', 'solver': 'adam'},\n",
        "    {'hidden_layer_sizes': (100,), 'activation': 'relu', 'solver': 'sgd'},\n",
        "    {'hidden_layer_sizes': (200, 100), 'activation': 'relu', 'solver': 'adam'},\n",
        "    {'hidden_layer_sizes': (50,), 'activation': 'relu', 'solver': 'adam'}\n",
        "]\n",
        "\n",
        "for idx, config in enumerate(configs, 1):\n",
        "    print(f\"Training Configuration {idx}...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    mlp = MLPClassifier(**config, max_iter=10, random_state=42)\n",
        "    mlp.fit(X_train, y_train)\n",
        "\n",
        "    train_pred = mlp.predict(X_train)\n",
        "    test_pred = mlp.predict(X_test)\n",
        "\n",
        "    train_acc = accuracy_score(y_train, train_pred)\n",
        "    test_acc = accuracy_score(y_test, test_pred)\n",
        "    duration = time.time() - start_time\n",
        "\n",
        "    print(f\"Config {idx}:\")\n",
        "    print(f\"  Layers: {config['hidden_layer_sizes']}, Activation: {config['activation']}, Solver: {config['solver']}\")\n",
        "    print(f\"  Training Accuracy: {train_acc * 100:.2f}%\")\n",
        "    print(f\"  Testing Accuracy: {test_acc * 100:.2f}%\")\n",
        "    print(f\"  Convergence Time: {duration:.2f} seconds\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvOEslibqoq7",
        "outputId": "3a1f6616-4196-4091-c1bd-348ce22759d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "\n",
            "=== Configuration 1: SGD without momentum ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy : 99.57%\n",
            "Testing Accuracy  : 97.85%\n",
            "Convergence Time  : 260.66 seconds\n",
            "Best Epoch        : 20\n",
            "Final Val Loss    : 0.0686\n",
            "\n",
            "=== Configuration 2: SGD with momentum (0.9) ===\n",
            "Training Accuracy : 100.00%\n",
            "Testing Accuracy  : 98.58%\n",
            "Convergence Time  : 356.94 seconds\n",
            "Best Epoch        : 4\n",
            "Final Val Loss    : 0.0635\n",
            "\n",
            "=== Configuration 3: Adam optimizer ===\n",
            "Training Accuracy : 99.66%\n",
            "Testing Accuracy  : 98.11%\n",
            "Convergence Time  : 382.50 seconds\n",
            "Best Epoch        : 7\n",
            "Final Val Loss    : 0.0726\n",
            "\n",
            "=== Configuration 4: SGD with Nesterov momentum ===\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Configurations\n",
        "configurations = [\n",
        "    {\"name\": \"Configuration 1: SGD without momentum\", \"optimizer\": SGD(learning_rate=0.01, momentum=0.0)},\n",
        "    {\"name\": \"Configuration 2: SGD with momentum (0.9)\", \"optimizer\": SGD(learning_rate=0.01, momentum=0.9)},\n",
        "    {\"name\": \"Configuration 3: Adam optimizer\", \"optimizer\": Adam()},\n",
        "    {\"name\": \"Configuration 4: SGD with Nesterov momentum\", \"optimizer\": SGD(learning_rate=0.01, momentum=0.9, nesterov=True)},\n",
        "    {\"name\": \"Configuration 5: Adam with modified learning rate (0.0005)\", \"optimizer\": Adam(learning_rate=0.0005)},\n",
        "]\n",
        "\n",
        "# Train and evaluate each configuration\n",
        "for config in configurations:\n",
        "    print(f\"\\n=== {config['name']} ===\")\n",
        "\n",
        "    # Define model\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=(28, 28)),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile model with specific optimizer\n",
        "    model.compile(optimizer=config[\"optimizer\"],\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Train model with timing\n",
        "    start_time = time.time()\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        epochs=20,\n",
        "                        batch_size=32,\n",
        "                        verbose=0)\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    # Evaluate model\n",
        "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "    train_acc = history.history['accuracy'][-1]\n",
        "    best_epoch = history.history['val_loss'].index(min(history.history['val_loss'])) + 1\n",
        "\n",
        "    # Output results\n",
        "    print(f\"Training Accuracy : {train_acc * 100:.2f}%\")\n",
        "    print(f\"Testing Accuracy  : {test_acc * 100:.2f}%\")\n",
        "    print(f\"Convergence Time  : {elapsed_time:.2f} seconds\")\n",
        "    print(f\"Best Epoch        : {best_epoch}\")\n",
        "    print(f\"Final Val Loss    : {min(history.history['val_loss']):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8iF_OlitA9A"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization, GaussianNoise\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "y_train, y_test = to_categorical(y_train), to_categorical(y_test)\n",
        "\n",
        "# Define model types\n",
        "models = {\n",
        "    \"Baseline\": lambda: Sequential([\n",
        "        Flatten(input_shape=(28, 28)),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(10, activation='softmax')\n",
        "    ]),\n",
        "\n",
        "    \"L2 Regularization\": lambda: Sequential([\n",
        "        Flatten(input_shape=(28, 28)),\n",
        "        Dense(512, activation='relu', kernel_regularizer='l2'),\n",
        "        Dense(256, activation='relu', kernel_regularizer='l2'),\n",
        "        Dense(128, activation='relu', kernel_regularizer='l2'),\n",
        "        Dense(10, activation='softmax')\n",
        "    ]),\n",
        "\n",
        "    \"Dropout\": lambda: Sequential([\n",
        "        Flatten(input_shape=(28, 28)),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(10, activation='softmax')\n",
        "    ]),\n",
        "\n",
        "    \"Batch Normalization\": lambda: Sequential([\n",
        "        Flatten(input_shape=(28, 28)),\n",
        "        Dense(512, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dense(256, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(10, activation='softmax')\n",
        "    ]),\n",
        "\n",
        "    \"Early Stopping\": lambda: Sequential([\n",
        "        Flatten(input_shape=(28, 28)),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(10, activation='softmax')\n",
        "    ]),\n",
        "\n",
        "    \"Input Noise Injection\": lambda: Sequential([\n",
        "        Flatten(input_shape=(28, 28)),\n",
        "        GaussianNoise(0.1),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(10, activation='softmax')\n",
        "    ]),\n",
        "\n",
        "    \"Hidden Layer Noise Injection\": lambda: Sequential([\n",
        "        Flatten(input_shape=(28, 28)),\n",
        "        Dense(512, activation='relu'),\n",
        "        GaussianNoise(0.05),\n",
        "        Dense(256, activation='relu'),\n",
        "        GaussianNoise(0.05),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(10, activation='softmax')\n",
        "    ]),\n",
        "\n",
        "    \"Output Layer Noise Injection\": lambda: Sequential([\n",
        "        Flatten(input_shape=(28, 28)),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(10),\n",
        "        GaussianNoise(0.1),\n",
        "        tf.keras.layers.Activation('softmax')\n",
        "    ])\n",
        "}\n",
        "\n",
        "\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
        "\n",
        "for name, build_fn in models.items():\n",
        "    print(f\"Training: {name}\")\n",
        "    model = build_fn()\n",
        "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    epochs = 50 if 'Early Stopping' in name else 20\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        validation_data=(x_test, y_test),\n",
        "                        epochs=epochs,\n",
        "                        batch_size=32,\n",
        "                        callbacks=[early_stop] if 'Early Stopping' in name else None,\n",
        "                        verbose=0)\n",
        "\n",
        "    train_acc = history.history['accuracy'][-1] * 100\n",
        "    test_acc = history.history['val_accuracy'][-1] * 100\n",
        "    overfit_gap = train_acc - test_acc\n",
        "\n",
        "    print(f\"Train Acc (%): {train_acc:.2f}\")\n",
        "    print(f\"Test Acc (%): {test_acc:.2f}\")\n",
        "    print(f\"Overfit Gap: {overfit_gap:.2f}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4p0YTo9XtKHe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Load and prepare data\n",
        "(x_train, _), (x_test, _) = datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 784).astype('float32') / 255.\n",
        "x_test = x_test.reshape(-1, 784).astype('float32') / 255.\n",
        "\n",
        "# 2. Add noise to test images\n",
        "noisy_test = x_test + 0.5 * np.random.normal(size=x_test.shape)\n",
        "noisy_test = np.clip(noisy_test, 0, 1)\n",
        "\n",
        "# 3. Simple autoencoder model\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(784, activation='sigmoid')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# 4. Train (using noisy training data)\n",
        "noisy_train = x_train + 0.5 * np.random.normal(size=x_train.shape)\n",
        "noisy_train = np.clip(noisy_train, 0, 1)\n",
        "model.fit(noisy_train, x_train, epochs=5, batch_size=256)\n",
        "\n",
        "# 5. Get denoised images\n",
        "denoised = model.predict(noisy_test)\n",
        "\n",
        "# 6. Show samples\n",
        "plt.figure(figsize=(10, 4))\n",
        "for i in range(5):  # show first 5 samples\n",
        "    # Original\n",
        "    plt.subplot(3, 5, i+1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28), cmap='gray')\n",
        "    plt.title(\"Original\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Noisy\n",
        "    plt.subplot(3, 5, i+6)\n",
        "    plt.imshow(noisy_test[i].reshape(28, 28), cmap='gray')\n",
        "    plt.title(\"Noisy\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Denoised\n",
        "    plt.subplot(3, 5, i+11)\n",
        "    plt.imshow(denoised[i].reshape(28, 28), cmap='gray')\n",
        "    plt.title(\"Denoised\")\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGxoZ5nOtQJi"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification, create_optimizer\n",
        "\n",
        "# Sample data\n",
        "texts = [\n",
        "    'Excellent product!',\n",
        "    'Poor quality',\n",
        "    'Highly recommend',\n",
        "    'Mediocre, not bad',\n",
        "    'Worst purchase ever',\n",
        "    'It works fine'\n",
        "]\n",
        "labels = [1, 0, 1, 0, 0, 1]  # 1 = Positive, 0 = Negative\n",
        "\n",
        "# Load tokenizer and tokenize texts\n",
        "tk = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "encodings = tk(texts, padding=True, truncation=True, max_length=128, return_tensors='tf')\n",
        "\n",
        "# Load model\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Set up optimizer\n",
        "opt, _ = create_optimizer(init_lr=2e-5, num_train_steps=3, num_warmup_steps=0)\n",
        "\n",
        "# Compile\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train\n",
        "model.fit(\n",
        "    {'input_ids': encodings['input_ids'], 'attention_mask': encodings['attention_mask']},\n",
        "    tf.convert_to_tensor(labels),\n",
        "    epochs=3\n",
        ")\n",
        "\n",
        "\n",
        "# Prediction function\n",
        "def predict(text):\n",
        "    inps = tk(text, return_tensors='tf', padding=True, truncation=True, max_length=128)\n",
        "    logits = model(inps).logits\n",
        "    probs = tf.nn.softmax(logits, axis=1)\n",
        "    pred = int(tf.argmax(probs, axis=1)[0])\n",
        "    conf = float(probs[0][pred])\n",
        "    return {\"sentiment\": \"Positive\" if pred == 1 else \"Negative\", \"confidence\": conf}\n",
        "\n",
        "# Test\n",
        "print(predict(\"Very good\"))  # Should return: {\"sentiment\": \"Positive\", \"confidence\": ~0.99}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bltWhMxvbg_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, GRU, Dense\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Hyperparameters\n",
        "num_words = 5000\n",
        "max_len = 100\n",
        "embedding_dim = 64\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)\n",
        "x_train, x_test = sequence.pad_sequences(x_train, maxlen=max_len), sequence.pad_sequences(x_test, maxlen=max_len)\n",
        "\n",
        "# Define RNN models\n",
        "def build_model(rnn_layer):\n",
        "    model = Sequential([\n",
        "        Embedding(num_words, embedding_dim, input_length=max_len),\n",
        "        rnn_layer,\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "models_config = [\n",
        "    {'name': 'SimpleRNN', 'layer': SimpleRNN(64)},\n",
        "    {'name': 'LSTM', 'layer': LSTM(64)},\n",
        "    {'name': 'GRU', 'layer': GRU(64)}\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "for config in models_config:\n",
        "    print(f\"\\n=== Training {config['name']} ===\")\n",
        "    \n",
        "    model = build_model(config['layer'])\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "    model.summary()\n",
        "\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        validation_split=0.2,\n",
        "                        epochs=3,\n",
        "                        batch_size=128,\n",
        "                        verbose=0)\n",
        "\n",
        "    # Evaluate on test set\n",
        "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "    # Store results\n",
        "    results.append({'name': config['name'], 'history': history, 'test_acc': test_acc})\n",
        "\n",
        "    # Sample prediction\n",
        "    sample_idx = [0, 1, 2]\n",
        "    sample_pred = model.predict(x_test[sample_idx])\n",
        "    print(\"Sample Predictions vs True Labels:\")\n",
        "    for i in sample_idx:\n",
        "        print(f\"Pred: {sample_pred[i][0]:.4f} | Actual: {y_test[i]}\")\n",
        "\n",
        "# Plot training performance comparison\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "for res in results:\n",
        "    plt.plot(res['history'].history['acc'], label=f\"{res['name']}_train\")\n",
        "    plt.plot(res['history'].history['val_acc'], '--', label=f\"{res['name']}_val\")\n",
        "plt.title(\"Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "for res in results:\n",
        "    plt.plot(res['history'].history['loss'], label=f\"{res['name']}_train\")\n",
        "    plt.plot(res['history'].history['val_loss'], '--', label=f\"{res['name']}_val\")\n",
        "plt.title(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Summary comparison\n",
        "print(\"\\n=== Model Comparison ===\")\n",
        "for res in results:\n",
        "    print(f\"{res['name']}: Test Acc = {res['test_acc']:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
